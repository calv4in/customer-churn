{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Analysis\n",
    "\n",
    "This notebook contains analysis of customer churn data to identify patterns and predictors of customer attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary libraries for data analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Interactive dashboard libraries\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set visualization styles\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading\n",
    "\n",
    "Load the customer churn dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Telco-Customer-Churn.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check data types and missing values\n",
    "print(\"\\nData types and non-null counts:\")\n",
    "display(df.info())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "display(df.isnull().sum())\n",
    "\n",
    "# Check unique values in categorical columns\n",
    "print(\"\\nUnique values in each column:\")\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "        print(df[col].value_counts().head(3))\n",
    "        print()\n",
    "\n",
    "# Check target variable distribution (class imbalance)\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "churn_distribution = df['Churn'].value_counts(normalize=True) * 100\n",
    "display(churn_distribution)\n",
    "print(f\"Class imbalance ratio: 1:{round(churn_distribution.iloc[0]/churn_distribution.iloc[1], 2)}\")\n",
    "\n",
    "# Plot the distribution of the target variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Churn', data=df)\n",
    "plt.title('Distribution of Customer Churn')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Checking multicollinearity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable distribution (class imbalance)\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "churn_distribution = df['Churn'].value_counts(normalize=True) * 100\n",
    "display(churn_distribution)\n",
    "print(f\"Class imbalance ratio: 1:{round(churn_distribution.iloc[0]/churn_distribution.iloc[1], 2)}\")\n",
    "\n",
    "# Create correlation matrix to check for multicollinearity\n",
    "# First, identify numerical columns\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "if 'Churn' in numeric_cols:\n",
    "    numeric_cols.remove('Churn')\n",
    "\n",
    "# Create correlation matrix for numerical features\n",
    "if len(numeric_cols) > 1:\n",
    "    print(\"\\nCorrelation matrix for numerical features (check for multicollinearity):\")\n",
    "    corr_matrix = df[numeric_cols].corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Matrix for Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Exploratory Data Analysis\n",
    "\n",
    "First, let's explore each feature's contribution to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - Part 1: Focus on key categorical features\n",
    "key_categorical_features = ['gender', 'Partner', 'Dependents', 'PhoneService', \n",
    "                           'InternetService', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
    "\n",
    "# Plot individual features vs churn rate\n",
    "for feature in key_categorical_features:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # Calculate the percentage of customers who churned for each category\n",
    "    # Use lambda function to convert Y/N to 1/0 (feature engineering)\n",
    "    churn_rate = df.groupby(feature)['Churn'].apply(lambda x: (x == 'Yes').mean() * 100)\n",
    "\n",
    "    \n",
    "    # Plot the bar chart\n",
    "    ax = churn_rate.plot(kind='bar', color='skyblue')\n",
    "    plt.title(f'Churn Rate by {feature}')\n",
    "    plt.ylabel('Churn Rate (%)')\n",
    "    plt.xlabel(feature)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, v in enumerate(churn_rate):\n",
    "        plt.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - Part 2: Numerical features\n",
    "numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "# Plot boxplots individually\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x='Churn', y=feature, data=df)\n",
    "    plt.title(f'{feature} by Churn Status')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# First, create the Churn_Binary column\n",
    "df['Churn_Binary'] = (df['Churn'] == 'Yes').astype(int)\n",
    "\n",
    "# Create tenure bins for easier interpretation\n",
    "df['tenure_group'] = pd.cut(df['tenure'], \n",
    "                           bins=[0, 12, 24, 36, 48, 60, 72], \n",
    "                           labels=['0-12', '13-24', '25-36', '37-48', '49-60', '61-72'])\n",
    "\n",
    "# Plot churn rate by tenure group\n",
    "plt.figure(figsize=(8, 5))\n",
    "tenure_churn = df.groupby('tenure_group')['Churn_Binary'].mean() * 100\n",
    "tenure_churn.plot(kind='bar', color='skyblue')\n",
    "plt.title('Churn Rate by Tenure Group (Months)')\n",
    "plt.ylabel('Churn Rate (%)')\n",
    "plt.xlabel('Tenure Group')\n",
    "for i, v in enumerate(tenure_churn):\n",
    "    plt.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - Part 3: Key insights plot\n",
    "\n",
    "# Contract type is usually the strongest predictor\n",
    "plt.figure(figsize=(8, 5))\n",
    "contract_churn = df.groupby('Contract')['Churn_Binary'].mean() * 100\n",
    "contract_churn.plot(kind='bar', color='skyblue')\n",
    "plt.title('Churn Rate by Contract Type')\n",
    "plt.ylabel('Churn Rate (%)')\n",
    "plt.xlabel('Contract Type')\n",
    "for i, v in enumerate(contract_churn):\n",
    "    plt.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Internet service type analysis\n",
    "plt.figure(figsize=(8, 5))\n",
    "internet_churn = df.groupby('InternetService')['Churn_Binary'].mean() * 100\n",
    "internet_churn.plot(kind='bar', color='skyblue')\n",
    "plt.title('Churn Rate by Internet Service Type')\n",
    "plt.ylabel('Churn Rate (%)')\n",
    "plt.xlabel('Internet Service')\n",
    "for i, v in enumerate(internet_churn):\n",
    "    plt.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Monthly charges distribution by churn status\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(data=df, x='MonthlyCharges', hue='Churn', bins=20, kde=True)\n",
    "plt.title('Monthly Charges Distribution by Churn Status')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Basic scatter plot - tenure vs monthly charges\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x='tenure', y='MonthlyCharges', hue='Churn', data=df)\n",
    "plt.title('Monthly Charges vs Tenure by Churn Status')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Visualizing Payment Methods vs Churn\n",
    "\n",
    "# Bar chart of churn rate by payment method\n",
    "plt.figure(figsize=(10, 6))\n",
    "payment_churn = df.groupby('PaymentMethod')['Churn_Binary'].mean() * 100\n",
    "payment_churn.sort_values(ascending=False).plot(kind='bar', color='skyblue')\n",
    "plt.title('Churn Rate by Payment Method', fontsize=14)\n",
    "plt.xlabel('Payment Method', fontsize=12)\n",
    "plt.ylabel('Churn Rate (%)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add percentage labels on top of bars\n",
    "for i, v in enumerate(payment_churn.sort_values(ascending=False)):\n",
    "    plt.text(i, v + 1, f\"{v:.1f}%\", ha='center', fontweight='bold')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count plot showing distribution of customers by payment method and churn\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = sns.countplot(x='PaymentMethod', hue='Churn', data=df, palette=['#3498db', '#e74c3c'])\n",
    "plt.title('Customer Distribution by Payment Method and Churn Status', fontsize=14)\n",
    "plt.xlabel('Payment Method', fontsize=12)\n",
    "plt.ylabel('Number of Customers', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Churn Status')\n",
    "\n",
    "# Add count labels on top of bars\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width()/2., height + 20, f'{height}', \n",
    "            ha = 'center', fontweight='bold')\n",
    "            \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Visualizing Additional Services vs Churn\n",
    "\n",
    "# List of additional service columns\n",
    "service_cols = ['PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup', \n",
    "                'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "\n",
    "# Create a figure with subplots for each service\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i, service in enumerate(service_cols, 1):\n",
    "    plt.subplot(4, 2, i)\n",
    "    \n",
    "    # Calculate churn rate for each category in the service\n",
    "    service_churn = df.groupby(service)['Churn_Binary'].mean() * 100\n",
    "    service_churn.sort_values(ascending=False).plot(kind='bar', color='lightgreen')\n",
    "    \n",
    "    plt.title(f'Churn Rate by {service}', fontsize=12)\n",
    "    plt.ylabel('Churn Rate (%)')\n",
    "    plt.ylim(0, service_churn.max() * 1.2)  # Set y-axis limit with some padding\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for j, v in enumerate(service_churn.sort_values(ascending=False)):\n",
    "        plt.text(j, v + 1, f\"{v:.1f}%\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Create a service heatmap to visualize combinations\n",
    "# First, let's look at internet-related services only\n",
    "internet_services = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', \n",
    "                     'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "\n",
    "# Create binary indicators for each service (1 if 'Yes', 0 otherwise)\n",
    "for service in internet_services:\n",
    "    df[f'{service}_Yes'] = (df[service] == 'Yes').astype(int)\n",
    "\n",
    "# Calculate service count for customers with internet\n",
    "internet_customers = df[df['InternetService'] != 'No']\n",
    "internet_customers['ServiceCount'] = internet_customers[[f'{service}_Yes' for service in internet_services]].sum(axis=1)\n",
    "\n",
    "# Plot churn rate by number of additional services\n",
    "plt.figure(figsize=(10, 6))\n",
    "service_count_churn = internet_customers.groupby('ServiceCount')['Churn_Binary'].mean() * 100\n",
    "service_count_churn.plot(kind='bar', color='#9b59b6')\n",
    "plt.title('Churn Rate by Number of Additional Internet Services', fontsize=14)\n",
    "plt.xlabel('Number of Additional Services', fontsize=12)\n",
    "plt.ylabel('Churn Rate (%)', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(service_count_churn):\n",
    "    plt.text(i, v + 1, f\"{v:.1f}%\", ha='center', fontweight='bold')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a heatmap of service combinations vs InternetService type\n",
    "plt.figure(figsize=(12, 8))\n",
    "pivot_table = df.pivot_table(values='Churn_Binary', \n",
    "                             index='InternetService',\n",
    "                             columns='Contract', \n",
    "                             aggfunc='mean') * 100\n",
    "\n",
    "sns.heatmap(pivot_table, annot=True, cmap='YlGnBu', fmt='.1f', linewidths=0.5)\n",
    "plt.title('Churn Rate (%) by Internet Service and Contract Type', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "\n",
    "# 1. Check for and handle missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# If 'TotalCharges' has missing values, they're likely new customers with tenure=0\n",
    "# Handle missing values in TotalCharges if any\n",
    "# Convert your TotalCharges column from object to numeric\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "if df['TotalCharges'].isnull().sum() > 0:\n",
    "    # Replace with 0 or MonthlyCharges based on business logic\n",
    "    df['TotalCharges'] = df['TotalCharges'].fillna(0)\n",
    "\n",
    "# 2. Convert 'Churn' column to binary\n",
    "if df['Churn'].dtype == 'object':\n",
    "    df['Churn_Binary'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "else:\n",
    "    df['Churn_Binary'] = df['Churn']\n",
    "\n",
    "# 3. Feature Engineering\n",
    "\n",
    "# Tenure feature engineering\n",
    "# Create tenure groups \n",
    "df['tenure'] = pd.to_numeric(df['tenure'])\n",
    "if 'tenure_group' not in df.columns:\n",
    "    df['tenure_group'] = pd.cut(df['tenure'], \n",
    "                              bins=[0, 12, 24, 36, 48, 60, 72], \n",
    "                              labels=['0-12', '13-24', '25-36', '37-48', '49-60', '61-72'])\n",
    "\n",
    "# Calculate monthly charges to total charges ratio (customer value)\n",
    "df['MonthlyToTotalRatio'] = df['MonthlyCharges'] / (df['TotalCharges'] + 1)  # Add 1 to avoid division by zero\n",
    "\n",
    "# Create service count feature for additional services\n",
    "service_cols = ['PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup', \n",
    "                'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "\n",
    "# Initialize a count column with zeros\n",
    "df['ServiceCount'] = 0\n",
    "\n",
    "# Count 'Yes' values in service columns\n",
    "for col in service_cols:\n",
    "    df['ServiceCount'] += (df[col] == 'Yes').astype(int)\n",
    "\n",
    "# Create internet service indicator\n",
    "df['HasInternetService'] = (df['InternetService'] != 'No').astype(int)\n",
    "\n",
    "# Create an interaction feature for contract type and tenure\n",
    "# First, encode contract type numerically\n",
    "df['ContractValue'] = df['Contract'].map({'Month-to-month': 0, 'One year': 1, 'Two year': 2})\n",
    "df['TenureContract'] = df['tenure'] * df['ContractValue']\n",
    "# This creates a new feature that combines tenure and contract type\n",
    "# Higher values indicate customers with longer tenure and longer contracts\n",
    "\n",
    "# Create binary features for each service\n",
    "for col in service_cols:\n",
    "    df[f'{col}_Yes'] = (df[col] == 'Yes').astype(int)\n",
    "# This converts categorical service columns to binary (0/1) features\n",
    "# Makes it easier for machine learning models to process\n",
    "\n",
    "# Print the new feature columns\n",
    "print(\"\\nFeature engineered columns:\")\n",
    "print([col for col in df.columns if col.startswith(('tenure_group', 'MonthlyToTotalRatio', 'ServiceCount', 'HasInternetService', 'ContractValue', 'TenureContract')) or '_Yes' in col])\n",
    "# This will print the newly created feature engineered columns\n",
    "\n",
    "# 4. Separate features and target variable\n",
    "y = df['Churn_Binary']\n",
    "\n",
    "# Identify categorical columns that need encoding\n",
    "categorical_cols = [col for col in df.select_dtypes(include=['object']).columns \n",
    "                   if col != 'Churn' and col != 'tenure_group']\n",
    "\n",
    "# Identify numerical columns for scaling\n",
    "numerical_cols = [col for col in df.select_dtypes(include=['float64', 'int64']).columns \n",
    "                 if col != 'Churn_Binary' and col != 'ContractValue']\n",
    "\n",
    "# Select features to use for modeling\n",
    "feature_cols = numerical_cols + categorical_cols\n",
    "\n",
    "X = df[feature_cols]\n",
    "\n",
    "print(\"\\nFeatures for modeling:\")\n",
    "print(feature_cols)\n",
    "\n",
    "# 5. Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "print(f\"Churn rate in training set: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Churn rate in testing set: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build preprocessing pipeline and baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a preprocessing pipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define preprocessing for numerical columns\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define preprocessing for categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Print preprocessing pipeline\n",
    "print(\"Preprocessing pipeline created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and evaluate baseline models\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "# Initialize models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    # Create a pipeline with preprocessing and the model\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate Precision-Recall curve and Average Precision\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # ROC curve subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Precision-Recall curve subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall, precision, color='green', lw=2, \n",
    "             label=f'Precision-Recall curve (AP = {avg_precision:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pipeline, accuracy, roc_auc, avg_precision\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    pipeline, accuracy, roc_auc, avg_precision = evaluate_model(\n",
    "        model, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    results[name] = {\n",
    "        'pipeline': pipeline,\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': roc_auc,\n",
    "        'avg_precision': avg_precision\n",
    "    }\n",
    "\n",
    "# Compare model performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[name]['accuracy'] for name in model_names]\n",
    "roc_aucs = [results[name]['roc_auc'] for name in model_names]\n",
    "avg_precisions = [results[name]['avg_precision'] for name in model_names]\n",
    "\n",
    "x = range(len(model_names))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar([i - width for i in x], accuracies, width, label='Accuracy', color='#3498db')\n",
    "plt.bar(x, roc_aucs, width, label='ROC AUC', color='#e74c3c')\n",
    "plt.bar([i + width for i in x], avg_precisions, width, label='Average Precision', color='#2ecc71')\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x, model_names)\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i - width, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "for i, v in enumerate(roc_aucs):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "for i, v in enumerate(avg_precisions):\n",
    "    plt.text(i + width, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: XGBoost Model Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"Optimizing XGBoost model...\")\n",
    "\n",
    "# Create the pipeline with XGBoost\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', xgb.XGBClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [3, 4, 5],\n",
    "    'model__learning_rate': [0.01, 0.1],\n",
    "    'model__subsample': [0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.8, 1.0],\n",
    "    'model__min_child_weight': [1, 5],\n",
    "    'model__gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Use a smaller grid for demonstration (faster execution)\n",
    "param_grid_small = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [3, 5],\n",
    "    'model__learning_rate': [0.1],\n",
    "}\n",
    "\n",
    "# Create grid search\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_pipeline,\n",
    "    param_grid_small,  # Use small grid for demonstration\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV score:\", grid_search.best_score_)\n",
    "\n",
    "# Get best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate best model\n",
    "print(\"\\nEvaluating optimized XGBoost model...\")\n",
    "_, accuracy, roc_auc, avg_precision = evaluate_model(\n",
    "    grid_search.best_estimator_.named_steps['model'], \n",
    "    X_train, X_test, y_train, y_test\n",
    ")\n",
    "\n",
    "# Store optimized XGBoost results\n",
    "results['Optimized XGBoost'] = {\n",
    "    'pipeline': best_xgb_model,\n",
    "    'accuracy': accuracy,\n",
    "    'roc_auc': roc_auc,\n",
    "    'avg_precision': avg_precision\n",
    "}\n",
    "\n",
    "# Feature importance for XGBoost\n",
    "best_xgb = best_xgb_model.named_steps['model']\n",
    "preprocessor = best_xgb_model.named_steps['preprocessor']\n",
    "\n",
    "# Transform the features\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = []\n",
    "\n",
    "# Get numerical feature names\n",
    "numerical_features = preprocessor.transformers_[0][2]\n",
    "feature_names.extend(numerical_features)\n",
    "\n",
    "# Get one-hot encoded categorical feature names\n",
    "categorical_features = preprocessor.transformers_[1][2]\n",
    "ohe = preprocessor.transformers_[1][1].named_steps['onehot']\n",
    "cat_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "feature_names.extend(cat_feature_names)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "importance = best_xgb.feature_importances_\n",
    "indices = np.argsort(importance)[-5:]  # Get top 5 features\n",
    "\n",
    "plt.barh(range(len(indices)), importance[indices], color='skyblue')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 5 Most Important Features (XGBoost)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Select best model for interactive dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best performing model based on accuracy\n",
    "best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
    "best_pipeline = results[best_model_name]['pipeline']\n",
    "best_accuracy = results[best_model_name]['accuracy']\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name} with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Save the model for later use in our dashboard\n",
    "import pickle\n",
    "\n",
    "# Create a dictionary with all necessary components for prediction\n",
    "model_components = {\n",
    "    'pipeline': best_pipeline,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'numerical_cols': numerical_cols,\n",
    "    'feature_cols': feature_cols\n",
    "}\n",
    "\n",
    "# Save to a file\n",
    "with open('telco_churn_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model_components, file)\n",
    "\n",
    "print(\"Model saved as 'telco_churn_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Build interactive dashboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}